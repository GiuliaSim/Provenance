{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "class Provenance:\n",
    "    \n",
    "    # Constants:\n",
    "    NAMESPACE_FUNC = 'activity:'\n",
    "    NAMESPACE_ENTITY = 'entity:'\n",
    "    INPUT = 'input'\n",
    "    OUTPUT = 'output'\n",
    "    \n",
    "    CHUNK_SIZE = 60000\n",
    "    \n",
    "    # PROV-N objects\n",
    "    ENTITY = 'prov:entity'\n",
    "    GENERATED_ENTITY = 'prov:generatedEntity'\n",
    "    USED_ENTITY = 'prov:usedEntity'\n",
    "    ACTIVITY = 'prov:activity'\n",
    "    \n",
    "    # PROV-N relations\n",
    "    GENERATION = 'wasGeneratedBy'\n",
    "    USE = 'used'\n",
    "    DERIVATION = 'wasDerivedFrom'\n",
    "    INVALIDATION = 'wasInvalidatedBy'\n",
    "    \n",
    "    def __init__(self, df, results_path=None):\n",
    "        # Inizialize provenance activities, relations and new entities\n",
    "        self.current_act = []\n",
    "        self.current_relations = []\n",
    "        self.new_entities = []\n",
    "        \n",
    "         # Set input dataframe parameters:\n",
    "        self.current_m, self.current_n = df.shape\n",
    "        self.current_columns = df.columns\n",
    "        self.current_index = df.index\n",
    "        \n",
    "        # Create provenance entities of the input dataframe:\n",
    "        self.current_ent = self.create_prov_entities(df, self.INPUT)\n",
    "        \n",
    "        # Initialize operation number:\n",
    "        self.operation_number = -1\n",
    "        self.instance = self.OUTPUT + str(self.operation_number)\n",
    "        \n",
    "        # Set results path:\n",
    "        self.results_path = 'results/' + time.strftime('%Y%m%d-%H%M%S') if results_path is None else results_path\n",
    "        \n",
    "        # Save input provenance document\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.INPUT))\n",
    "        \n",
    "    def create_entity(self, ent_id, record_id, value, feature_name, instance):\n",
    "        \"\"\"Create a provenance entity.\n",
    "        Return a dictionary with the id and the attributes of the entity.\"\"\"\n",
    "        # Get attributes:\n",
    "        other_attributes = {}\n",
    "        other_attributes['record_id'] = record_id\n",
    "        other_attributes['value'] = value\n",
    "        other_attributes['feature_name'] = feature_name\n",
    "        #other_attributes['instance'] = instance\n",
    "        \n",
    "        # Add entity to new numpy array entities:\n",
    "        ent = {'identifier': ent_id, 'attributes': other_attributes}\n",
    "        self.new_entities.append(ent)\n",
    "        \n",
    "        return ent\n",
    "    \n",
    "    def create_activity(self, function_name, features_name=None, other_attributes=None):\n",
    "        \"\"\"Create a provenance activity and add to the current activities array.\n",
    "        Return the id of the new prov activity.\"\"\"\n",
    "        # Get default activity attributes:\n",
    "        attributes = {}\n",
    "        attributes['function_name'] = function_name\n",
    "        if features_name is not None:\n",
    "            attributes['features_name'] =  features_name\n",
    "        attributes['operation_number'] = str(self.operation_number)\n",
    "        \n",
    "        # Join default and extra attributes:\n",
    "        if other_attributes is not None:\n",
    "            attributes.update(other_attributes)\n",
    "            \n",
    "        act_id = self.NAMESPACE_FUNC + str(uuid.uuid4())\n",
    "        \n",
    "        # Add activity to current provenance document:\n",
    "        act = {'identifier': act_id, 'attributes': attributes}\n",
    "        self.current_act.append(act)\n",
    "        \n",
    "        return act_id\n",
    "    \n",
    "    def create_relation(self, relation_type, **relation):\n",
    "        \"\"\"Add a relation to the current relations array.\n",
    "        Return the new relation.\"\"\"\n",
    "        if relation_type == self.GENERATION:\n",
    "            relation[self.ENTITY] = relation.pop('a')\n",
    "            relation[self.ACTIVITY] = relation.pop('b')\n",
    "        elif relation_type == self.USE:\n",
    "            relation[self.ACTIVITY] = relation.pop('a')\n",
    "            relation[self.ENTITY] = relation.pop('b')\n",
    "        elif relation_type == self.DERIVATION:\n",
    "            relation[self.GENERATED_ENTITY] = relation.pop('a')\n",
    "            relation[self.USED_ENTITY] = relation.pop('b')\n",
    "        elif relation_type == self.INVALIDATION:\n",
    "            relation[self.ENTITY] = relation.pop('a')\n",
    "            relation[self.ACTIVITY] = relation.pop('b')\n",
    "        #else:\n",
    "            #TODO: Exception, invalid relation\n",
    "            \n",
    "        relation.update({'prov:relation_type':relation_type})\n",
    "        self.current_relations.append(relation)\n",
    "        \n",
    "        return relation\n",
    "        \n",
    "    def create_prov_entities(self, dataframe, instance=None):\n",
    "        \"\"\"Return a numpy array of new provenance entities related to the dataframe.\"\"\"\n",
    "        instance = self.instance if instance is None else instance\n",
    "        columns = dataframe.columns\n",
    "        \n",
    "        # Copy input values in array\n",
    "        # values = np.array(dataframe.values)\n",
    "        # Create a function that adds input entities to prov document\n",
    "        # createEntities = lambda i,j: self.create_entity(self.NAMESPACE_ENTITY + str(uuid.uuid4()), \n",
    "        #                                                  str(values[i][j]), \n",
    "        #                                                  columns[j], \n",
    "        #                                                  instance)\n",
    "        # entities = np.fromfunction(np.vectorize(createEntities), values.shape, dtype=object)\n",
    "        \n",
    "        # Create output array of entities:\n",
    "        entities = np.empty(dataframe.shape, dtype=object)\n",
    "        for i in range(self.current_m):\n",
    "            record_id = str(uuid.uuid4())\n",
    "            for j in range(self.current_n):\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                value = str(dataframe.iat[i, j])\n",
    "                 # Add entity to current provenance document:\n",
    "                entities[i][j] = self.create_entity(ent_id, record_id, value, columns[j], instance)\n",
    "\n",
    "        return entities\n",
    "    \n",
    "    def set_current_values(self, dataframe, entities_out):\n",
    "        \"\"\"Update values of current entities after every operation.\"\"\"\n",
    "        # Set output dataframe entities:\n",
    "        self.current_m, self.current_n = dataframe.shape\n",
    "        self.current_columns = dataframe.columns\n",
    "        self.current_index = dataframe.index\n",
    "        self.current_ent = entities_out\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.current_act = []\n",
    "        self.current_relations = []\n",
    "        self.new_entities = []\n",
    "\n",
    "        # Increment operation number:\n",
    "        self.operation_number += 1\n",
    "        self.instance = self.OUTPUT + str(self.operation_number)\n",
    "            \n",
    "    def save_json_prov(self, nameFile):\n",
    "        \"\"\"Save provenance in json file.\"\"\"\n",
    "        if not os.path.exists(nameFile):\n",
    "            os.makedirs(nameFile)\n",
    "        \n",
    "        ents_path = os.path.join(nameFile, 'entities')\n",
    "        acts_path = os.path.join(nameFile, 'activities.json')\n",
    "        rel_path = os.path.join(nameFile, 'relations')\n",
    "                \n",
    "        # Save entities: \n",
    "        #entities = list(self.current_ent.flatten())\n",
    "        entities = self.new_entities\n",
    "        \n",
    "        if entities:\n",
    "            for i in range(0, len(entities), self.CHUNK_SIZE):\n",
    "                output_name = ents_path + '.json' if i//self.CHUNK_SIZE == 0 else ents_path + '_' + str(i//self.CHUNK_SIZE) + '.json'\n",
    "                with open(output_name, 'w', encoding='utf-8') as ents_file:\n",
    "                    ents = entities[i:i+self.CHUNK_SIZE]\n",
    "                    json.dump(ents, ents_file, ensure_ascii=False, indent=4)\n",
    "                \n",
    "        # Save activities:\n",
    "        if self.current_act:\n",
    "            with open(acts_path, 'w', encoding='utf-8') as acts_file:\n",
    "                json.dump(self.current_act, acts_file, ensure_ascii=False, indent=4)\n",
    "                \n",
    "        # Save all relations:\n",
    "        if self.current_relations:\n",
    "            for i in range(0, len(self.current_relations), self.CHUNK_SIZE):\n",
    "                output_name = rel_path + '.json' if i//self.CHUNK_SIZE == 0 else rel_path + '_' + str(i//self.CHUNK_SIZE) + '.json'\n",
    "                with open(output_name, 'w', encoding='utf-8') as rel_file:\n",
    "                    rels = self.current_relations[i:i+self.CHUNK_SIZE]\n",
    "                    json.dump(rels, rel_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    def timing(f):\n",
    "        def wrap(*args):\n",
    "            # Get timing of provenance function:\n",
    "            time1 = time.time()\n",
    "            ret = f(*args)\n",
    "            time2 = time.time()\n",
    "            text = '{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0)\n",
    "            print(text)\n",
    "            \n",
    "            self = args[0]\n",
    "            path = os.path.join(self.results_path, 'log_file.txt')\n",
    "            with open(path, 'a+') as log_file:\n",
    "                log_file.write('[' + time.strftime(\"%d/%m-%H:%M:%S\") +']' + text + '\\n')\n",
    "            \n",
    "            #duration = time2 - time1\n",
    "            #print(f.__name__\n",
    "            #      + ' finished in ' \n",
    "            #      + time.strftime('%H:%M:%S', time.gmtime(duration)))\n",
    "\n",
    "            return ret\n",
    "        return wrap\n",
    "    \n",
    "        \n",
    "    ###\n",
    "    ###  PROVENANCE METHODS\n",
    "    ###\n",
    "\n",
    "    @timing\n",
    "    def get_prov_binarizer(self, df_out, columnsName, function_name='Binarizer'): \n",
    "        \"\"\"Return provenance document related to binarization function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of binarized columns name\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        \n",
    "        for col_name in columnsName:\n",
    "            act_id = self.create_activity(function_name, col_name)\n",
    "            col_index = columns_out.get_loc(col_name)\n",
    "            for i in range(self.current_m):\n",
    "                e_in = entities_in[i][col_index]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                record_id = e_in['attributes']['record_id']\n",
    "                value = str(df_out.iat[i, col_index])\n",
    "                \n",
    "                # Create a new entity with new value:\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                e_out = self.create_entity(ent_id, record_id, value, col_name, self.instance)\n",
    "                e_out_identifier = e_out['identifier']\n",
    "                    \n",
    "                self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                self.create_relation(self.USE, a=act_id, b=e_in_identifier)\n",
    "                self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                    \n",
    "                entities_in[i][col_index] = e_out        \n",
    "                \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in) \n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, str(self.instance)))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timing\n",
    "    def get_prov_feature_transformation(self, df_out, columnsName, function_name='Feature Transformation'):\n",
    "        \"\"\"Return provenance document related to features trasformation function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of transformed columns name\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        \n",
    "        for col_name in columnsName:\n",
    "            act_id = self.create_activity(function_name, col_name)\n",
    "            col_index = columns_out.get_loc(col_name)\n",
    "            for i in range(self.current_m):\n",
    "                e_in = entities_in[i][col_index]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                record_id = e_in['attributes']['record_id']\n",
    "                value = str(df_out.iat[i, col_index])\n",
    "                \n",
    "                # Create a new entity with new value:\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                e_out = self.create_entity(ent_id, record_id, value, col_name, self.instance)\n",
    "                e_out_identifier = e_out['identifier']\n",
    "                    \n",
    "                self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                self.create_relation(self.USE, a=act_id, b=e_in_identifier)\n",
    "                self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                    \n",
    "                entities_in[i][col_index] = e_out\n",
    "                \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    @timing\n",
    "    def get_prov_space_transformation(self, df_out, columnsName, function_name='Space Transformation'):\n",
    "        \"\"\"Return provenance document related to space trasformation function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of columns name joined to create the new column\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        m, n = self.current_m, self.current_n\n",
    "\n",
    "        # Output values:\n",
    "        m_new, n_new = df_out.shape\n",
    "        columns_out = df_out.columns\n",
    "        \n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        new_entities = np.empty((m, n_new-n), dtype=object)\n",
    "        \n",
    "        # Get feature indexes used for space transformation:\n",
    "        indexes = []\n",
    "        for feature in columnsName:\n",
    "            indexes.append(columns_out.get_loc(feature))\n",
    "            \n",
    "        # Create space transformation activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(columnsName))\n",
    "        \n",
    "        # Get provenance related to existent data:\n",
    "        for i in range(m):\n",
    "            for j in indexes:\n",
    "                e_in = entities_in[i][j]\n",
    "                ent_id = e_in['identifier']\n",
    "                self.create_relation(self.USE, a=act_id, b=ent_id)\n",
    "                    \n",
    "        # Get provenance related to the new column:\n",
    "        for i in range(m):\n",
    "            first_ent = entities_in[i][0]\n",
    "            record_id = first_ent['attributes']['record_id']\n",
    "            for j in range(n, n_new):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                e_out = self.create_entity(ent_id, record_id, value, columns_out[j], self.instance)\n",
    "                e_out_identifier = e_out['identifier']\n",
    "                new_entities[i][j-n] = e_out\n",
    "                self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                for index in indexes:\n",
    "                    e_in = entities_in[i][index]\n",
    "                    e_in_identifier = e_in['identifier']\n",
    "                    self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                    \n",
    "        entities_out = np.concatenate((entities_in, new_entities), axis=1)\n",
    "                \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "\n",
    "        return self\n",
    "\n",
    "    @timing\n",
    "    def get_prov_feature_selection(self, df_out, function_name='Feature Selection'):\n",
    "        \"\"\"Return provenance document related to feature selection function.\"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        m_new, n_new = df_out.shape\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "\n",
    "        delColumnsName = set(columns_in) - set(columns_out)  # List of selected columns\n",
    "        \n",
    "        # Create feature selection activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(delColumnsName))\n",
    "        \n",
    "        delColumns = []\n",
    "        for colName in delColumnsName:\n",
    "            j = columns_in.get_loc(colName)\n",
    "            delColumns.append(j)\n",
    "            for i in range(m):\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                self.create_relation(self.INVALIDATION, a=e_in_identifier, b=act_id)\n",
    "        \n",
    "        entities_out = np.delete(entities_in, delColumns, axis=1)\n",
    "                    \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @timing\n",
    "    def get_prov_dim_reduction(self, df_out, function_name='Dimensionality reduction'):\n",
    "        \"\"\"Return provenance document related to selection or projection.\"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "        index_in = self.current_index\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        index_out = df_out.index\n",
    "        m_new, n_new = df_out.shape\n",
    "        # Create entities of the output dataframe:\n",
    "        # entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        delColumnsName = set(columns_in) - set(columns_out) # List of deleted columns\n",
    "        delIndex = set(index_in) - set(index_out) # List of deleted columns\n",
    "        \n",
    "        # Create selection activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(delColumnsName))\n",
    "        \n",
    "        for i in delIndex:\n",
    "            for j in range(n):\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                self.create_relation(self.INVALIDATION, a=e_in_identifier, b=act_id)\n",
    "        \n",
    "        delColumns = []\n",
    "        for colName in delColumnsName:\n",
    "            j = columns_in.get_loc(colName)\n",
    "            delColumns.append(j)\n",
    "            for i in range(m):\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                self.create_relation(self.INVALIDATION, a=e_in_identifier, b=act_id)\n",
    "        \n",
    "        entities_in = np.delete(entities_in, list(delIndex), axis=0)\n",
    "        entities_out = np.delete(entities_in, delColumns, axis=1)\n",
    "        \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "\n",
    "        return self\n",
    "\n",
    "    @timing\n",
    "    def get_prov_instance_generation(self, df_out, function_name='Instance Generation'):\n",
    "        \"\"\"Return provenance document related to instance generation function.\"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        m_new, n_new = df_out.shape\n",
    "        \n",
    "        # Create numpy array of new entities:\n",
    "        #entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        new_entities = np.empty((m_new-m, n), dtype=object)\n",
    "        \n",
    "        acts = []\n",
    "        for j in range(n):\n",
    "            # Create function for every columns\n",
    "            act_id = self.create_activity(function_name, columns_out[j])\n",
    "            acts.append(act_id)\n",
    "            \n",
    "        for i in range(m_new):\n",
    "            record_id = str(uuid.uuid4())\n",
    "            for j in range(n):\n",
    "                act_id = acts[j]\n",
    "                value = str(df_out.iat[i, j])\n",
    "                if i < m:\n",
    "                    # Provenance of existent data\n",
    "                    e_in = entities_in[i][j]\n",
    "                    ent_id = e_in['identifier']\n",
    "                    self.create_relation(self.USE, a=act_id, b=ent_id)\n",
    "                else:\n",
    "                    # Provenance of new data\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, record_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    new_entities[i-m][j] = e_out\n",
    "                    self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                    \n",
    "        entities_out = np.concatenate((entities_in, new_entities), axis=0)\n",
    "        \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "\n",
    "        return self\n",
    "\n",
    "    @timing\n",
    "    def get_prov_onehot_encode(self, df_out, onehot_cols, onehot_cols_map, function_name='OneHot Encoding'):\n",
    "        \"\"\"Return provenance document related to one-hot encoding function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        onehot_cols -- list of One-Hot encoded columns \n",
    "        onehot_cols_map -- map(key, values)\n",
    "                           where key is the One-Hot encoded column name\n",
    "                           and values is an array of the new columns name\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "\n",
    "        # Output values:\n",
    "        m_new, n_new = df_out.shape\n",
    "        columns_out = df_out.columns\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        activities_dict = {}\n",
    "        \n",
    "        # Create functions (one for all one hot encoded feature):\n",
    "        for col_name in onehot_cols:\n",
    "            act_id = self.create_activity(function_name, col_name)\n",
    "            activities_dict[col_name] = act_id\n",
    "            \n",
    "            # Add input entities used by functions:\n",
    "            column_index = columns_in.get_loc(col_name) if col_name in columns_in else -1\n",
    "            for i in range(self.current_m):\n",
    "                e_in = entities_in[i][column_index]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                self.create_relation(self.USE, a=act_id, b=e_in_identifier)\n",
    "                if col_name not in columns_out:\n",
    "                    self.create_relation(self.INVALIDATION, a=e_in_identifier, b=act_id)\n",
    "                \n",
    "        new_columns = set(columns_out) - set(columns_in)\n",
    "        \n",
    "        # Add generated entities:\n",
    "        for i in range(self.current_m):\n",
    "            first_ent = entities_in[i][0]\n",
    "            record_id = first_ent['attributes']['record_id']\n",
    "            for column_out_name in new_columns:\n",
    "                j = columns_out.get_loc(column_out_name)\n",
    "                value = str(df_out.iat[i, j])\n",
    "                for k,v in onehot_cols_map.items():\n",
    "                    if column_out_name in v:\n",
    "                        column_name = k\n",
    "                    \n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                e_out = self.create_entity(ent_id, record_id, value, column_out_name, self.instance)\n",
    "                e_out_identifier = e_out['identifier']\n",
    "                activity = activities_dict[column_name]\n",
    "                j_old = columns_in.get_loc(column_name)\n",
    "                e_in = entities_in[i][j_old]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                self.create_relation(self.GENERATION, a=e_out_identifier, b=activity)\n",
    "                self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                \n",
    "                entities_out[i][j] = e_out\n",
    "        \n",
    "        # Rearrange unchanged columns:\n",
    "        for col_name in columns_out:\n",
    "            if col_name in columns_in:\n",
    "                old_j = columns_in.get_loc(col_name)\n",
    "                new_j = columns_out.get_loc(col_name)\n",
    "                entities_out[:,new_j] = entities_in[:,old_j]\n",
    "        \n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "        \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @timing\n",
    "    def get_prov_value_transformation(self, df_out, value, function_name='Value Transformation'):\n",
    "        \"\"\"Return provenance document related to value transformation function.\n",
    "        Used when a value inside the dataframe is replaced.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        value -- replaced value\n",
    "        \"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "\n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        \n",
    "        # Create value transformation activity:\n",
    "        act_id = self.create_activity(function_name)\n",
    "        \n",
    "        for i in range(self.current_m):\n",
    "            for j in range(self.current_n):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                \n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                record_id = e_in['attributes']['record_id']\n",
    "                val_in = e_in['attributes']['value']\n",
    "                \n",
    "                # Check if the input value is the replaced value\n",
    "                if str(val_in) != str(value):\n",
    "                    # Create new entity with the new value\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, record_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                    self.create_relation(self.USE, a=act_id, b=e_in_identifier)\n",
    "                    self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                    entities_in[i][j] = e_out\n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # TODO: imputation relativo alla singola colonna o a tutte insieme?\n",
    "    #       Puo comprendere un sottoinsieme di colonne o tutte insieme?\n",
    "    @timing\n",
    "    def get_prov_imputation(self, df_out, isIndipendent=True, function_name='Imputation'):\n",
    "        \"\"\"Return provenance document related to imputation function.\"\"\"\n",
    "        self.initialize()\n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        \n",
    "        # Create a single imputation activity if the imputation is related to a single column:\n",
    "        if isIndipendent:\n",
    "            act_id = self.create_activity(function_name) \n",
    "            \n",
    "        for j in range(self.current_n):\n",
    "            if not isIndipendent:\n",
    "                act_id = self.create_activity(function_name) \n",
    "            for i in range(self.current_m):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                \n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                record_id = e_in['attributes']['record_id']\n",
    "                val_in = e_in['attributes']['value']\n",
    "                \n",
    "                if val_in == 'nan':\n",
    "                    # Create new entity with the new value\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, record_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    self.create_relation(self.GENERATION, a=e_out_identifier, b=act_id)\n",
    "                    self.create_relation(self.USE, a=act_id, b=e_in_identifier)\n",
    "                    self.create_relation(self.DERIVATION, a=e_out_identifier, b=e_in_identifier)\n",
    "                    entities_in[i][j] = e_out\n",
    "                    \n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

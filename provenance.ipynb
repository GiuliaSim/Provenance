{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov.model as prov\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from prov.dot import prov_to_dot\n",
    "\n",
    "class Provenance:\n",
    "    \n",
    "    # Constants:\n",
    "    NAMESPACE_FUNC = 'function:'\n",
    "    NAMESPACE_ENTITY = 'entity:'\n",
    "    INPUT = 'input'\n",
    "    OUTPUT = 'output'\n",
    "    CHUNK_SIZE = 30000\n",
    "    \n",
    "    def __init__(self, df, results_path=None, split_json=False):\n",
    "         # Set input dataframe parameters:\n",
    "        self.current_m, self.current_n = df.shape\n",
    "        self.current_columns = df.columns\n",
    "        self.current_index = df.index\n",
    "    \n",
    "        # Create a new provenance document:\n",
    "        self.current_provDoc = self.create_prov_document()\n",
    "        \n",
    "        # Create provenance entities of the input dataframe:\n",
    "        self.current_ent = self.create_prov_entities(df, self.INPUT)\n",
    "        \n",
    "        # Initialize operation number:\n",
    "        self.operation_number = 0\n",
    "        self.instance = self.OUTPUT + str(self.operation_number)\n",
    "        \n",
    "        # Set results path:\n",
    "        results_path = 'results/' + time.strftime('%Y%m%d-%H%M%S') if results_path is None else results_path\n",
    "        self.results_path = results_path\n",
    "        self.split_json = split_json\n",
    "        \n",
    "        # Save input provenance document\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.INPUT))\n",
    "        \n",
    "    def create_prov_document(self):\n",
    "        \"\"\"Return a new emplty provenance document.\"\"\"\n",
    "        doc = prov.ProvDocument()  # doc is now an empty provenance document\n",
    "        doc.set_default_namespace('default/')       # Default namespace \n",
    "        doc.add_namespace('function', 'function/')  # Add namespace for functions\n",
    "        doc.add_namespace('entity', 'entity/')      # Add namespace for entities\n",
    "        # Set doc as current provenance document:\n",
    "        self.current_provDoc = doc\n",
    "        return doc\n",
    "        \n",
    "    def create_entity(self, ent_id, value, feature_name, instance):\n",
    "        \"\"\"Add an entity to the current provenance document.\n",
    "        Return a dictionary with the id and the attributes of the entity.\"\"\"\n",
    "        # Get attributes:\n",
    "        other_attributes = {}\n",
    "        other_attributes['value'] = value\n",
    "        other_attributes['feature name'] = feature_name\n",
    "        other_attributes['instance'] = instance\n",
    "        \n",
    "        # Add entity to current provenance document:\n",
    "        entity = self.current_provDoc.entity(ent_id, other_attributes)\n",
    "        \n",
    "        return {'identifier': ent_id, 'attributes': other_attributes}\n",
    "    \n",
    "    def create_activity(self, function_name, features_name=None, other_attributes=None):\n",
    "        \"\"\"Add an activity to the current provenance document.\n",
    "        Return the id of the new prov activity.\"\"\"\n",
    "        # Get default activity attributes:\n",
    "        attributes = {}\n",
    "        attributes['function name'] = function_name\n",
    "        if features_name is not None:\n",
    "            attributes['features name'] =  features_name\n",
    "        attributes['operation number'] = str(self.operation_number)\n",
    "        \n",
    "        # Join default and extra attributes:\n",
    "        if other_attributes is not None:\n",
    "            attributes.update(other_attributes)\n",
    "            \n",
    "        act_id = self.NAMESPACE_FUNC + str(uuid.uuid4())\n",
    "        \n",
    "        # Add activity to current provenance document:\n",
    "        act_id = self.current_provDoc.activity(act_id, None, None, attributes)\n",
    "        \n",
    "        return act_id\n",
    "        #return act\n",
    "    \n",
    "    def add_ent(self, elem, instance, feature_name):\n",
    "        ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "        return self.create_entity(ent_id, elem, feature_name, instance)\n",
    "    \n",
    "    def create_entities(self, dataframe, instance=None):\n",
    "        \"\"\"Return a numpy array of new provenance entities related to the dataframe.\"\"\"\n",
    "        instance = self.instance if instance is None else instance\n",
    "        columns = dataframe.columns\n",
    "        newdf = pd.DataFrame(columns, dtype=object)\n",
    "        for i in range(self.current_n):\n",
    "            feature_name = columns[i]\n",
    "            newdf[feature_name] = dataframe[feature_name].apply(self.add_ent, args=[instance, feature_name])\n",
    "        return newdf\n",
    "        \n",
    "    def create_prov_entities(self, dataframe, instance=None):\n",
    "        \"\"\"Return a numpy array of new provenance entities related to the dataframe.\"\"\"\n",
    "        instance = self.instance if instance is None else instance\n",
    "        columns = dataframe.columns\n",
    "        \n",
    "        # Copy input values in array\n",
    "        # values = np.array(dataframe.values)\n",
    "        # Create a function that adds input entities to prov document\n",
    "        # createEntities = lambda i,j: self.create_entity(self.NAMESPACE_ENTITY + str(uuid.uuid4()), \n",
    "        #                                                  str(values[i][j]), \n",
    "        #                                                  columns[j], \n",
    "        #                                                  instance)\n",
    "        # entities = np.fromfunction(np.vectorize(createEntities), values.shape, dtype=object)\n",
    "        \n",
    "        # Create output array of entities:\n",
    "        entities = np.empty(dataframe.shape, dtype=object)\n",
    "        for i in range(self.current_m):\n",
    "            for j in range(self.current_n):\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                value = str(dataframe.iat[i, j])\n",
    "                 # Add entity to current provenance document:\n",
    "                entities[i][j] = self.create_entity(ent_id, value, columns[j], instance)\n",
    "\n",
    "        return entities\n",
    "    \n",
    "    def set_current_values(self, dataframe, entities_out):\n",
    "        \"\"\"Update values of current entities after every operation.\"\"\"\n",
    "        # Set output dataframe entities:\n",
    "        self.current_m, self.current_n = dataframe.shape\n",
    "        self.current_columns = dataframe.columns\n",
    "        self.current_index = dataframe.index\n",
    "        self.current_ent = entities_out\n",
    "        \n",
    "        # Increment operation number:\n",
    "        self.operation_number += 1\n",
    "        self.instance = self.OUTPUT + str(self.operation_number)\n",
    "        \n",
    "    def split_json_file(self, nameFile):\n",
    "        json_path = nameFile + '.json'\n",
    "        if not os.path.exists(nameFile):\n",
    "            os.makedirs(nameFile)\n",
    "        ents_path = os.path.join(nameFile, 'entities')\n",
    "        acts_path = os.path.join(nameFile, 'activities.json')\n",
    "        conn_path = os.path.join(nameFile, 'connections.json')\n",
    "        with open(json_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            data.pop('prefix', None) # delete prefix infos\n",
    "                \n",
    "            # Save entities:\n",
    "            entities = data['entity']\n",
    "            \n",
    "            for k, v in entities.items():\n",
    "                v.update({'id':k})\n",
    "                \n",
    "            for i in range(0, len(entities), self.CHUNK_SIZE):\n",
    "                output_name = ents_path + '_' + str(i//self.CHUNK_SIZE) + '.json'\n",
    "                with open(output_name, 'w', encoding='utf-8') as ents_file:\n",
    "                    #ents = dict(list(entities.items())[i:i+self.CHUNK_SIZE])\n",
    "                    ents = list(entities.values())[i:i+self.CHUNK_SIZE]\n",
    "                    json.dump(ents, ents_file, ensure_ascii=False, indent=4)\n",
    "            data.pop('entity', None)\n",
    "            #with open(ents_path, 'w', encoding='utf-8') as ents_file:\n",
    "                #json.dump(entities, ents_file, ensure_ascii=False, indent=4)\n",
    "                #data.pop('entity', None)\n",
    "                    \n",
    "            # Save activities:\n",
    "            if 'activity' in data:\n",
    "                with open(acts_path, 'w', encoding='utf-8') as acts_file:\n",
    "                    activities = data['activity']\n",
    "                    for k, v in activities.items():\n",
    "                        v.update({'id':k})\n",
    "                    json.dump(list(activities.values()), acts_file, ensure_ascii=False, indent=4)\n",
    "                    data.pop('activity', None)\n",
    "                \n",
    "            # Save all connections:\n",
    "            if data:\n",
    "                with open(conn_path, 'w', encoding='utf-8') as conn_file:\n",
    "                    json.dump(data, conn_file, ensure_ascii=False, indent=4)  \n",
    "        \n",
    "    def save_json_prov(self, nameFile):\n",
    "        \"\"\"Save provenance in json file.\"\"\"\n",
    "        prov_doc = self.current_provDoc\n",
    "        directory = os.path.dirname(nameFile)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        prov_doc.serialize(nameFile + '.json', indent=2)\n",
    "        \n",
    "        if self.split_json:\n",
    "            self.split_json_file(nameFile)\n",
    "        \n",
    "    def save_graph(self, nameFile):\n",
    "        \"\"\"Save provenance of last operation in png image graph.\"\"\"\n",
    "        prov_doc = self.current_provDoc\n",
    "        dot = prov_to_dot(prov_doc)\n",
    "        dot.write_png(nameFile + '.png')\n",
    "        \n",
    "    def save_all_graph(self, nameFile):\n",
    "        \"\"\"Save all provenance in png image graph.\"\"\"\n",
    "        directory = os.path.dirname(nameFile)\n",
    "        final_doc = prov.ProvDocument()\n",
    "        prov_doc = prov.ProvDocument()\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith('.json'):\n",
    "                prov_doc = prov_doc.deserialize(os.path.join(directory, file))\n",
    "                final_doc.update(prov_doc)\n",
    "        dot = prov_to_dot(final_doc)\n",
    "        dot.write_png(nameFile + '.png')\n",
    "    \n",
    "    def timing(f):\n",
    "        def wrap(*args):\n",
    "            time1 = time.time()\n",
    "            ret = f(*args)\n",
    "            time2 = time.time()\n",
    "            print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "            \n",
    "        # Get timing of provenance function:\n",
    "#         duration = time2 - time1\n",
    "#         print(f.__name__\n",
    "#               + ' finished in ' \n",
    "#               + time.strftime('%H:%M:%S', time.gmtime(duration)))\n",
    "\n",
    "            return ret\n",
    "        return wrap\n",
    "    \n",
    "        \n",
    "    ###\n",
    "    ###  PROVENANCE METHODS\n",
    "    ###\n",
    "\n",
    "    @timing\n",
    "    def getProv_Binarizer(self, df_out, columnsName, function_name='Binarizer'): \n",
    "        \"\"\"Return provenance document related to binarization function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of binarized columns name\n",
    "        \"\"\"\n",
    "        prov_doc = self.create_prov_document()  # Create a new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        # entities_out = np.empty(df_out.shape, dtype=object)\n",
    "                \n",
    "        for j in range(self.current_n):\n",
    "            # Create activity for all binarized columns:\n",
    "            if columns_out[j] in columnsName:\n",
    "                act_id = self.create_activity(function_name, columns_out[j])\n",
    "            for i in range(self.current_m):\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                value = str(df_out.iat[i, j])\n",
    "                if columns_out[j] in columnsName:\n",
    "                    # Create a new entity with new value:\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    \n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "                    prov_doc.used(act_id, e_in_identifier)\n",
    "                    prov_doc.wasDerivedFrom(e_out_identifier, e_in_identifier)\n",
    "                else:\n",
    "                    # Add new instance to the original entity:\n",
    "                    ent_id = e_in_identifier\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    \n",
    "                entities_in[i][j] = e_out\n",
    "                    \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, str(self.instance)))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in) \n",
    "        \n",
    "        return prov_doc\n",
    "    \n",
    "    @timing\n",
    "    def getProv_FeatureTransformation(self, df_out, columnsName, function_name='Feature Transformation'):\n",
    "        \"\"\"Return provenance document related to features trasformation function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of transformed columns name\n",
    "        \"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        # entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        for j in range(self.current_n):\n",
    "            # Create activity for all trasformed columns:\n",
    "            if columns_out[j] in columnsName:\n",
    "                act_id = self.create_activity(function_name, columns_out[j])\n",
    "            for i in range(self.current_m):\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                value = str(df_out.iat[i, j])\n",
    "                if columns_out[j] in columnsName:\n",
    "                    # Create a new entity with new value:\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    \n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "                    prov_doc.used(act_id, e_in_identifier)\n",
    "                    prov_doc.wasDerivedFrom(e_out_identifier, e_in_identifier)\n",
    "                else:\n",
    "                    # Add new instance to the original entity:\n",
    "                    ent_id = e_in_identifier\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    \n",
    "                entities_in[i][j] = e_out\n",
    "                    \n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "        \n",
    "        return prov_doc\n",
    "\n",
    "    @timing\n",
    "    def getProv_SpaceTransformation(self, df_out, columnsName, function_name='Space Transformation'):\n",
    "        \"\"\"Return provenance document related to space trasformation function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        columnsName -- list of columns name joined to create the new column\n",
    "        \"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Get feature indexes used for space transformation:\n",
    "        indexes = []\n",
    "        for feature in columnsName:\n",
    "            indexes.append(df_out.columns.get_loc(feature))\n",
    "\n",
    "        # Output values:\n",
    "        m_new, n_new = df_out.shape\n",
    "        columns_out = df_out.columns\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "\n",
    "        # Create space transformation activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(columnsName))\n",
    "        \n",
    "        # Get provenance related to existent data:\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                e_in = entities_in[i][j]\n",
    "                ent_id = e_in['identifier']\n",
    "                e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                entities_out[i][j] = e_out\n",
    "                if j in indexes:\n",
    "                    prov_doc.used(act_id, ent_id)\n",
    "                    \n",
    "        # Get provenance related to the new column:\n",
    "        for i in range(m):\n",
    "            for j in range(n, n_new):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                e_out_identifier = e_out['identifier']\n",
    "                entities_out[i][j] = e_out\n",
    "                prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "                for index in indexes:\n",
    "                    e_in = entities_in[i][index]\n",
    "                    e_in_identifier = e_in['identifier']\n",
    "                    prov_doc.wasDerivedFrom(e_out_identifier, e_in_identifier)\n",
    "                \n",
    "                \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "\n",
    "        return prov_doc\n",
    "\n",
    "    @timing\n",
    "    def getProv_FeatureSelection(self, df_out, function_name='Feature Selection'):\n",
    "        \"\"\"Return provenance document related to feature selection function.\"\"\"\n",
    "        prov_doc = self.create_prov_document()  # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        m_new, n_new = df_out.shape\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "\n",
    "        columnsName = set(columns_in) - set(columns_out)  # List of selected columns\n",
    "        \n",
    "        # Create feature selection activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(columnsName))\n",
    "        \n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                new_column_index = columns_out.get_loc(columns_in[j]) if columns_in[j] in columns_out else -1\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                if new_column_index == -1:\n",
    "                    prov_doc.wasInvalidatedBy(e_in_identifier, act_id)\n",
    "                else:\n",
    "                    value = str(df_out.iat[i, new_column_index])\n",
    "                    e_out = self.create_entity(e_in_identifier, value, columns_out[new_column_index], self.instance)\n",
    "                    entities_out[i][new_column_index] = e_out\n",
    "                    \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "\n",
    "        return prov_doc\n",
    "    \n",
    "    @timing\n",
    "    def getProv_Drop(self, df_out, function_name='Drop'):\n",
    "        \"\"\"Return provenance document related to drop function.\"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "        index_in = self.current_index\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        index_out = df_out.index\n",
    "        m_new, n_new = df_out.shape\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        columnsName = set(columns_in) - set(columns_out) # List of selected columns\n",
    "        \n",
    "        # Create drop activity:\n",
    "        act_id = self.create_activity(function_name, ', '.join(columnsName))\n",
    "        \n",
    "        for i in range(m):\n",
    "            new_row_index = index_out.get_loc(index_in[i]) if index_in[i] in index_out else -1\n",
    "            for j in range(n):\n",
    "                new_column_index = columns_out.get_loc(columns_in[j])  if columns_in[j] in columns_out else -1\n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                if new_row_index == -1 or new_column_index == -1:\n",
    "                    prov_doc.wasInvalidatedBy(e_in_identifier, act_id)\n",
    "                else:\n",
    "                    value = str(df_out.iat[new_row_index, new_column_index])\n",
    "                    e_out = self.create_entity(e_in_identifier, value, columns_out[new_column_index], self.instance)\n",
    "                    entities_out[new_row_index][new_column_index] = e_out\n",
    "                    \n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "\n",
    "        return prov_doc\n",
    "\n",
    "    @timing\n",
    "    def getProv_InstanceGeneration(self, df_out, function_name='Instance Generation'):\n",
    "        \"\"\"Return provenance document related to instance generation function.\"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        m, n = self.current_m, self.current_n\n",
    "        \n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        m_new, n_new = df_out.shape\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        for j in range(n_new):\n",
    "            # Create function for every columns\n",
    "            act_id = self.create_activity(function_name, columns_out[j])\n",
    "            for i in range(m_new):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                if i < m:\n",
    "                    # Provenance of existent data\n",
    "                    e_in = entities_in[i][j]\n",
    "                    ent_id = e_in['identifier']\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    entities_out[i][j] = e_out\n",
    "                    prov_doc.used(act_id, ent_id)\n",
    "                else:\n",
    "                    # Provenance of new data\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    entities_out[i][j] = e_out\n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "\n",
    "        return prov_doc\n",
    "\n",
    "    @timing\n",
    "    def getProv_OneHotEncode(self, df_out, onehot_cols, onehot_cols_map, function_name='OneHot Encoding'):\n",
    "        \"\"\"Return provenance document related to one-hot encoding function.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        onehot_cols -- list of One-Hot encoded columns \n",
    "        onehot_cols_map -- map(key, values)\n",
    "                           where key is the One-Hot encoded column name\n",
    "                           and values is an array of the new columns name\n",
    "        \"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        columns_in = self.current_columns\n",
    "\n",
    "        # Output values:\n",
    "        m_new, n_new = df_out.shape\n",
    "        columns_out = df_out.columns\n",
    "        # Create entities of the output dataframe:\n",
    "        entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        activities_dict = {}\n",
    "\n",
    "        # Get One-Hot provenance:\n",
    "        for j in range(n_new):\n",
    "            column_out_name = columns_out[j]\n",
    "            new_column_index = columns_in.get_loc(column_out_name) if column_out_name in columns_in else -1\n",
    "            # Create functions (one for all one hot encoded feature)\n",
    "            if j < self.current_n and columns_in[j] in onehot_cols:\n",
    "                    act_id = self.create_activity(function_name, columns_in[j])\n",
    "                    activities_dict[columns_in[j]] = act_id\n",
    "                    \n",
    "            for i in range(m_new):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                # Unchanged output:\n",
    "                if column_out_name in columns_in:\n",
    "                    e_in = entities_in[i][new_column_index]\n",
    "                    ent_id = e_in['identifier']\n",
    "                    e_out = self.create_entity(ent_id, value, column_out_name, self.instance)\n",
    "                # New data:\n",
    "                else:\n",
    "                    for k,v in onehot_cols_map.items():\n",
    "                        if column_out_name in v:\n",
    "                            column_name = k\n",
    "                    \n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, column_out_name, self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    activity = activities_dict[column_name]\n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, activity)\n",
    "\n",
    "                # Add input entities used by functions:\n",
    "                if j < self.current_n and columns_in[j] in onehot_cols:\n",
    "                    e_in = entities_in[i][j]\n",
    "                    e_in_identifier = e_in['identifier']\n",
    "                    prov_doc.used(act_id, e_in_identifier)\n",
    "                    \n",
    "                entities_out[i][j] = e_out\n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_out)\n",
    "\n",
    "        return prov_doc\n",
    "    \n",
    "    @timing\n",
    "    def getProv_ValueTransformation(self, df_out, value, function_name='Value Transformation'):\n",
    "        \"\"\"Return provenance document related to value transformation function.\n",
    "        Used when a value inside the dataframe is replaced.\n",
    "        \n",
    "        Keyword argument:\n",
    "        df_out -- the output dataframe\n",
    "        value -- replaced value\n",
    "        \"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "\n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        # Create entities of the output dataframe:\n",
    "        # entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        # Create value transformation activity:\n",
    "        act_id = self.create_activity(function_name)\n",
    "        \n",
    "        for i in range(self.current_m):\n",
    "            for j in range(self.current_n):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                \n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                val_in = e_in['attributes']['value']\n",
    "                \n",
    "                # Check if the input value is the replaced value\n",
    "                if str(val_in) == str(value):\n",
    "                    # Add new instance to the original entity:\n",
    "                    ent_id = e_in['identifier']\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                else:\n",
    "                    # Create new entity with the new value\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    prov_doc.wasDerivedFrom(e_out_identifier, e_in_identifier)\n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "                    prov_doc.used(act_id, e_in_identifier)\n",
    "                    \n",
    "                entities_in[i][j] = e_out\n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "\n",
    "        return prov_doc\n",
    "    \n",
    "    # TODO: imputation relativo alla singola colonna o a tutte insieme?\n",
    "    #       Puo comprendere un sottoinsieme di colonne o tutte insieme?\n",
    "    @timing\n",
    "    def getProv_Imputation(self, df_out, isSingleAct=True, function_name='Imputation'):\n",
    "        \"\"\"Return provenance document related to imputation function.\"\"\"\n",
    "        prov_doc = self.create_prov_document() # Create new provenance document\n",
    "        \n",
    "        # Get current values:\n",
    "        entities_in = self.current_ent\n",
    "        # Output values:\n",
    "        columns_out = df_out.columns\n",
    "        # Create entities of the output dataframe:\n",
    "        # entities_out = np.empty(df_out.shape, dtype=object)\n",
    "        \n",
    "        # Create a single imputation activity if the imputation is related to a single column:\n",
    "        if isSingleAct:\n",
    "            act_id = self.create_activity(function_name) \n",
    "            \n",
    "        for j in range(self.current_n):\n",
    "            if not isSingleAct:\n",
    "                act_id = self.create_activity(function_name) \n",
    "            for i in range(self.current_m):\n",
    "                value = str(df_out.iat[i, j])\n",
    "                \n",
    "                e_in = entities_in[i][j]\n",
    "                e_in_identifier = e_in['identifier']\n",
    "                val_in = e_in['attributes']['value']\n",
    "                \n",
    "                if val_in == 'nan':\n",
    "                    # Create new entity with the new value\n",
    "                    ent_id = self.NAMESPACE_ENTITY + str(uuid.uuid4())\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                    e_out_identifier = e_out['identifier']\n",
    "                    prov_doc.wasGeneratedBy(e_out_identifier, act_id)\n",
    "                    prov_doc.used(act_id, e_in_identifier)\n",
    "                    prov_doc.wasDerivedFrom(e_out_identifier, e_in_identifier)\n",
    "                else:\n",
    "                    # Add new instance to the original entity:\n",
    "                    ent_id = e_in['identifier']\n",
    "                    e_out = self.create_entity(ent_id, value, columns_out[j], self.instance)\n",
    "                \n",
    "                entities_in[i][j] = e_out\n",
    "                    \n",
    "\n",
    "        # Save provenance document in json file:\n",
    "        self.save_json_prov(os.path.join(self.results_path, self.instance))\n",
    "        # Update current values:\n",
    "        self.set_current_values(df_out, entities_in)\n",
    "\n",
    "        return prov_doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
